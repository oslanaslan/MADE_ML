{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-86e0de040aac317a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Lab assignment №1, part 2\n",
    "\n",
    "This lab assignment consists of several parts. You are supposed to make some transformations, train some models, estimate the quality of the models and explain your results.\n",
    "\n",
    "Several comments:\n",
    "* Don't hesitate to ask questions, it's a good practice.\n",
    "* No private/public sharing, please. The copied assignments will be graded with 0 points.\n",
    "* Blocks of this lab will be graded separately."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__*This is the second part of the assignment. First and third parts are waiting for you in the same directory.*__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-512ba712fc0fc065",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## Part 2. Data preprocessing, model training and evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-b656a4266174b009",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### 1. Reading the data\n",
    "Today we work with the [dataset](https://archive.ics.uci.edu/ml/datasets/Statlog+%28Vehicle+Silhouettes%29), describing different cars for multiclass ($k=4$) classification problem. The data is available below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If on colab, uncomment the following lines\n",
    "\n",
    "# ! wget https://raw.githubusercontent.com/girafe-ai/ml-mipt/21f_made/homeworks/Lab1_ML_pipeline_and_SVM/car_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-eebac6bfdf73d0bc",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(846, 19) (846,)\n",
      "(549, 19) (549,) (297, 19) (297,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = pd.read_csv('car_data.csv', delimiter=',', header=None).values\n",
    "data = dataset[:, :-1].astype(int)\n",
    "target = dataset[:, -1]\n",
    "\n",
    "print(data.shape, target.shape)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, target, test_size=0.35)\n",
    "print(X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-88b1a0f688568f2c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "To get some insights about the dataset, `pandas` might be used. The `train` part is transformed to `pd.DataFrame` below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>398</td>\n",
       "      <td>98</td>\n",
       "      <td>38</td>\n",
       "      <td>70</td>\n",
       "      <td>186</td>\n",
       "      <td>68</td>\n",
       "      <td>6</td>\n",
       "      <td>164</td>\n",
       "      <td>39</td>\n",
       "      <td>20</td>\n",
       "      <td>136</td>\n",
       "      <td>189</td>\n",
       "      <td>413</td>\n",
       "      <td>129</td>\n",
       "      <td>71</td>\n",
       "      <td>3</td>\n",
       "      <td>17</td>\n",
       "      <td>200</td>\n",
       "      <td>203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>147</td>\n",
       "      <td>91</td>\n",
       "      <td>40</td>\n",
       "      <td>76</td>\n",
       "      <td>171</td>\n",
       "      <td>67</td>\n",
       "      <td>7</td>\n",
       "      <td>149</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>135</td>\n",
       "      <td>169</td>\n",
       "      <td>332</td>\n",
       "      <td>144</td>\n",
       "      <td>68</td>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>192</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>345</td>\n",
       "      <td>101</td>\n",
       "      <td>54</td>\n",
       "      <td>106</td>\n",
       "      <td>188</td>\n",
       "      <td>57</td>\n",
       "      <td>7</td>\n",
       "      <td>236</td>\n",
       "      <td>28</td>\n",
       "      <td>26</td>\n",
       "      <td>164</td>\n",
       "      <td>256</td>\n",
       "      <td>833</td>\n",
       "      <td>253</td>\n",
       "      <td>81</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>185</td>\n",
       "      <td>185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>485</td>\n",
       "      <td>83</td>\n",
       "      <td>46</td>\n",
       "      <td>71</td>\n",
       "      <td>156</td>\n",
       "      <td>70</td>\n",
       "      <td>6</td>\n",
       "      <td>151</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>147</td>\n",
       "      <td>174</td>\n",
       "      <td>338</td>\n",
       "      <td>198</td>\n",
       "      <td>80</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>181</td>\n",
       "      <td>186</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>39</td>\n",
       "      <td>81</td>\n",
       "      <td>45</td>\n",
       "      <td>68</td>\n",
       "      <td>169</td>\n",
       "      <td>73</td>\n",
       "      <td>6</td>\n",
       "      <td>151</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>146</td>\n",
       "      <td>173</td>\n",
       "      <td>336</td>\n",
       "      <td>186</td>\n",
       "      <td>75</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>183</td>\n",
       "      <td>189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>748</td>\n",
       "      <td>93</td>\n",
       "      <td>46</td>\n",
       "      <td>85</td>\n",
       "      <td>169</td>\n",
       "      <td>66</td>\n",
       "      <td>9</td>\n",
       "      <td>151</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>147</td>\n",
       "      <td>169</td>\n",
       "      <td>339</td>\n",
       "      <td>179</td>\n",
       "      <td>67</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>195</td>\n",
       "      <td>204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>338</td>\n",
       "      <td>97</td>\n",
       "      <td>45</td>\n",
       "      <td>91</td>\n",
       "      <td>161</td>\n",
       "      <td>63</td>\n",
       "      <td>10</td>\n",
       "      <td>151</td>\n",
       "      <td>45</td>\n",
       "      <td>19</td>\n",
       "      <td>148</td>\n",
       "      <td>166</td>\n",
       "      <td>334</td>\n",
       "      <td>171</td>\n",
       "      <td>65</td>\n",
       "      <td>18</td>\n",
       "      <td>20</td>\n",
       "      <td>197</td>\n",
       "      <td>205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>216</td>\n",
       "      <td>84</td>\n",
       "      <td>44</td>\n",
       "      <td>77</td>\n",
       "      <td>150</td>\n",
       "      <td>59</td>\n",
       "      <td>5</td>\n",
       "      <td>152</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>143</td>\n",
       "      <td>175</td>\n",
       "      <td>344</td>\n",
       "      <td>177</td>\n",
       "      <td>77</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>183</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>474</td>\n",
       "      <td>82</td>\n",
       "      <td>40</td>\n",
       "      <td>73</td>\n",
       "      <td>141</td>\n",
       "      <td>57</td>\n",
       "      <td>8</td>\n",
       "      <td>153</td>\n",
       "      <td>44</td>\n",
       "      <td>19</td>\n",
       "      <td>133</td>\n",
       "      <td>173</td>\n",
       "      <td>342</td>\n",
       "      <td>153</td>\n",
       "      <td>75</td>\n",
       "      <td>11</td>\n",
       "      <td>9</td>\n",
       "      <td>181</td>\n",
       "      <td>187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>610</td>\n",
       "      <td>103</td>\n",
       "      <td>56</td>\n",
       "      <td>105</td>\n",
       "      <td>183</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>210</td>\n",
       "      <td>32</td>\n",
       "      <td>24</td>\n",
       "      <td>173</td>\n",
       "      <td>217</td>\n",
       "      <td>648</td>\n",
       "      <td>218</td>\n",
       "      <td>72</td>\n",
       "      <td>13</td>\n",
       "      <td>22</td>\n",
       "      <td>188</td>\n",
       "      <td>196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>605</td>\n",
       "      <td>97</td>\n",
       "      <td>53</td>\n",
       "      <td>105</td>\n",
       "      <td>225</td>\n",
       "      <td>71</td>\n",
       "      <td>12</td>\n",
       "      <td>221</td>\n",
       "      <td>30</td>\n",
       "      <td>25</td>\n",
       "      <td>167</td>\n",
       "      <td>226</td>\n",
       "      <td>713</td>\n",
       "      <td>202</td>\n",
       "      <td>70</td>\n",
       "      <td>3</td>\n",
       "      <td>20</td>\n",
       "      <td>186</td>\n",
       "      <td>200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>316</td>\n",
       "      <td>91</td>\n",
       "      <td>41</td>\n",
       "      <td>66</td>\n",
       "      <td>131</td>\n",
       "      <td>56</td>\n",
       "      <td>9</td>\n",
       "      <td>126</td>\n",
       "      <td>53</td>\n",
       "      <td>18</td>\n",
       "      <td>144</td>\n",
       "      <td>159</td>\n",
       "      <td>237</td>\n",
       "      <td>155</td>\n",
       "      <td>72</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>191</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>782</td>\n",
       "      <td>87</td>\n",
       "      <td>44</td>\n",
       "      <td>98</td>\n",
       "      <td>211</td>\n",
       "      <td>70</td>\n",
       "      <td>10</td>\n",
       "      <td>189</td>\n",
       "      <td>35</td>\n",
       "      <td>22</td>\n",
       "      <td>141</td>\n",
       "      <td>214</td>\n",
       "      <td>535</td>\n",
       "      <td>178</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>21</td>\n",
       "      <td>187</td>\n",
       "      <td>194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>227</td>\n",
       "      <td>94</td>\n",
       "      <td>35</td>\n",
       "      <td>66</td>\n",
       "      <td>147</td>\n",
       "      <td>62</td>\n",
       "      <td>9</td>\n",
       "      <td>131</td>\n",
       "      <td>50</td>\n",
       "      <td>18</td>\n",
       "      <td>127</td>\n",
       "      <td>159</td>\n",
       "      <td>258</td>\n",
       "      <td>115</td>\n",
       "      <td>66</td>\n",
       "      <td>8</td>\n",
       "      <td>7</td>\n",
       "      <td>196</td>\n",
       "      <td>201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>557</td>\n",
       "      <td>86</td>\n",
       "      <td>40</td>\n",
       "      <td>66</td>\n",
       "      <td>138</td>\n",
       "      <td>59</td>\n",
       "      <td>4</td>\n",
       "      <td>137</td>\n",
       "      <td>49</td>\n",
       "      <td>18</td>\n",
       "      <td>133</td>\n",
       "      <td>162</td>\n",
       "      <td>279</td>\n",
       "      <td>151</td>\n",
       "      <td>74</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>186</td>\n",
       "      <td>190</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     0    1   2    3    4   5   6    7   8   9    10   11   12   13  14  15  \\\n",
       "0   398   98  38   70  186  68   6  164  39  20  136  189  413  129  71   3   \n",
       "1   147   91  40   76  171  67   7  149  44  19  135  169  332  144  68   4   \n",
       "2   345  101  54  106  188  57   7  236  28  26  164  256  833  253  81   6   \n",
       "3   485   83  46   71  156  70   6  151  44  19  147  174  338  198  80   3   \n",
       "4    39   81  45   68  169  73   6  151  44  19  146  173  336  186  75   7   \n",
       "5   748   93  46   85  169  66   9  151  44  19  147  169  339  179  67   0   \n",
       "6   338   97  45   91  161  63  10  151  45  19  148  166  334  171  65  18   \n",
       "7   216   84  44   77  150  59   5  152  44  19  143  175  344  177  77   8   \n",
       "8   474   82  40   73  141  57   8  153  44  19  133  173  342  153  75  11   \n",
       "9   610  103  56  105  183  59  10  210  32  24  173  217  648  218  72  13   \n",
       "10  605   97  53  105  225  71  12  221  30  25  167  226  713  202  70   3   \n",
       "11  316   91  41   66  131  56   9  126  53  18  144  159  237  155  72   3   \n",
       "12  782   87  44   98  211  70  10  189  35  22  141  214  535  178  71   2   \n",
       "13  227   94  35   66  147  62   9  131  50  18  127  159  258  115  66   8   \n",
       "14  557   86  40   66  138  59   4  137  49  18  133  162  279  151  74   6   \n",
       "\n",
       "    16   17   18  \n",
       "0   17  200  203  \n",
       "1   17  192  200  \n",
       "2   14  185  185  \n",
       "3   11  181  186  \n",
       "4    0  183  189  \n",
       "5    4  195  204  \n",
       "6   20  197  205  \n",
       "7    2  183  187  \n",
       "8    9  181  187  \n",
       "9   22  188  196  \n",
       "10  20  186  200  \n",
       "11  10  191  194  \n",
       "12  21  187  194  \n",
       "13   7  196  201  \n",
       "14  14  186  190  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd = pd.DataFrame(X_train)\n",
    "\n",
    "# First 15 rows of our dataset.\n",
    "X_train_pd.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-98e7d91d77d65fcf",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Methods `describe` and `info` deliver some useful information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.00000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "      <td>549.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>411.216758</td>\n",
       "      <td>93.264117</td>\n",
       "      <td>44.584699</td>\n",
       "      <td>81.329690</td>\n",
       "      <td>167.43898</td>\n",
       "      <td>61.701275</td>\n",
       "      <td>8.566485</td>\n",
       "      <td>166.714026</td>\n",
       "      <td>41.324226</td>\n",
       "      <td>20.406193</td>\n",
       "      <td>147.429872</td>\n",
       "      <td>186.692168</td>\n",
       "      <td>427.934426</td>\n",
       "      <td>173.251366</td>\n",
       "      <td>72.324226</td>\n",
       "      <td>6.420765</td>\n",
       "      <td>12.347905</td>\n",
       "      <td>188.938069</td>\n",
       "      <td>195.586521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>244.662210</td>\n",
       "      <td>7.989124</td>\n",
       "      <td>6.002632</td>\n",
       "      <td>15.404955</td>\n",
       "      <td>32.30690</td>\n",
       "      <td>7.362983</td>\n",
       "      <td>4.545958</td>\n",
       "      <td>31.627569</td>\n",
       "      <td>7.535837</td>\n",
       "      <td>2.454111</td>\n",
       "      <td>14.085411</td>\n",
       "      <td>29.363534</td>\n",
       "      <td>167.134882</td>\n",
       "      <td>31.668369</td>\n",
       "      <td>7.082451</td>\n",
       "      <td>4.868677</td>\n",
       "      <td>8.749406</td>\n",
       "      <td>6.192752</td>\n",
       "      <td>7.380822</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>73.000000</td>\n",
       "      <td>33.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>105.00000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>26.000000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>130.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>109.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>176.000000</td>\n",
       "      <td>181.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>203.000000</td>\n",
       "      <td>87.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>69.000000</td>\n",
       "      <td>141.00000</td>\n",
       "      <td>57.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>34.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>137.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>317.000000</td>\n",
       "      <td>148.000000</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>184.000000</td>\n",
       "      <td>190.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>398.000000</td>\n",
       "      <td>92.000000</td>\n",
       "      <td>44.000000</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>164.00000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>155.000000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>19.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>177.000000</td>\n",
       "      <td>355.000000</td>\n",
       "      <td>173.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>11.000000</td>\n",
       "      <td>188.000000</td>\n",
       "      <td>197.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>625.000000</td>\n",
       "      <td>99.000000</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>96.000000</td>\n",
       "      <td>194.00000</td>\n",
       "      <td>65.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>192.000000</td>\n",
       "      <td>46.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>158.000000</td>\n",
       "      <td>213.000000</td>\n",
       "      <td>557.000000</td>\n",
       "      <td>194.000000</td>\n",
       "      <td>75.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>193.000000</td>\n",
       "      <td>201.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>845.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>59.000000</td>\n",
       "      <td>112.000000</td>\n",
       "      <td>306.00000</td>\n",
       "      <td>126.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>262.000000</td>\n",
       "      <td>61.000000</td>\n",
       "      <td>28.000000</td>\n",
       "      <td>186.000000</td>\n",
       "      <td>285.000000</td>\n",
       "      <td>998.000000</td>\n",
       "      <td>264.000000</td>\n",
       "      <td>119.000000</td>\n",
       "      <td>22.000000</td>\n",
       "      <td>41.000000</td>\n",
       "      <td>206.000000</td>\n",
       "      <td>211.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3          4           5   \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.00000  549.000000   \n",
       "mean   411.216758   93.264117   44.584699   81.329690  167.43898   61.701275   \n",
       "std    244.662210    7.989124    6.002632   15.404955   32.30690    7.362983   \n",
       "min      0.000000   73.000000   33.000000   40.000000  105.00000   47.000000   \n",
       "25%    203.000000   87.000000   40.000000   69.000000  141.00000   57.000000   \n",
       "50%    398.000000   92.000000   44.000000   78.000000  164.00000   61.000000   \n",
       "75%    625.000000   99.000000   49.000000   96.000000  194.00000   65.000000   \n",
       "max    845.000000  119.000000   59.000000  112.000000  306.00000  126.000000   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean     8.566485  166.714026   41.324226   20.406193  147.429872  186.692168   \n",
       "std      4.545958   31.627569    7.535837    2.454111   14.085411   29.363534   \n",
       "min      3.000000  112.000000   26.000000   17.000000  118.000000  130.000000   \n",
       "25%      7.000000  146.000000   34.000000   19.000000  137.000000  167.000000   \n",
       "50%      8.000000  155.000000   43.000000   19.000000  145.000000  177.000000   \n",
       "75%     10.000000  192.000000   46.000000   22.000000  158.000000  213.000000   \n",
       "max     55.000000  262.000000   61.000000   28.000000  186.000000  285.000000   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  549.000000  549.000000  549.000000  549.000000  549.000000  549.000000   \n",
       "mean   427.934426  173.251366   72.324226    6.420765   12.347905  188.938069   \n",
       "std    167.134882   31.668369    7.082451    4.868677    8.749406    6.192752   \n",
       "min    184.000000  109.000000   60.000000    0.000000    0.000000  176.000000   \n",
       "25%    317.000000  148.000000   67.000000    2.000000    5.000000  184.000000   \n",
       "50%    355.000000  173.000000   72.000000    6.000000   11.000000  188.000000   \n",
       "75%    557.000000  194.000000   75.000000    9.000000   18.000000  193.000000   \n",
       "max    998.000000  264.000000  119.000000   22.000000   41.000000  206.000000   \n",
       "\n",
       "               18  \n",
       "count  549.000000  \n",
       "mean   195.586521  \n",
       "std      7.380822  \n",
       "min    181.000000  \n",
       "25%    190.000000  \n",
       "50%    197.000000  \n",
       "75%    201.000000  \n",
       "max    211.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_pd.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 549 entries, 0 to 548\n",
      "Data columns (total 19 columns):\n",
      " #   Column  Non-Null Count  Dtype\n",
      "---  ------  --------------  -----\n",
      " 0   0       549 non-null    int32\n",
      " 1   1       549 non-null    int32\n",
      " 2   2       549 non-null    int32\n",
      " 3   3       549 non-null    int32\n",
      " 4   4       549 non-null    int32\n",
      " 5   5       549 non-null    int32\n",
      " 6   6       549 non-null    int32\n",
      " 7   7       549 non-null    int32\n",
      " 8   8       549 non-null    int32\n",
      " 9   9       549 non-null    int32\n",
      " 10  10      549 non-null    int32\n",
      " 11  11      549 non-null    int32\n",
      " 12  12      549 non-null    int32\n",
      " 13  13      549 non-null    int32\n",
      " 14  14      549 non-null    int32\n",
      " 15  15      549 non-null    int32\n",
      " 16  16      549 non-null    int32\n",
      " 17  17      549 non-null    int32\n",
      " 18  18      549 non-null    int32\n",
      "dtypes: int32(19)\n",
      "memory usage: 40.9 KB\n"
     ]
    }
   ],
   "source": [
    "X_train_pd.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-be844269be69c387",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### 2. Machine Learning pipeline\n",
    "Here you are supposed to perform the desired transformations. Please, explain your results briefly after each task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.0. Data preprocessing\n",
    "* Make some transformations of the dataset (if necessary). Briefly explain the transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### My code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первым делом попробую понять есть ли в данных категориальные фичи. Для этого посчитаю количество уникальных значений в каждой колонке."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-a1514aa189a49fca",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values in 0: 549 (100%)\n",
      "Unique values in 1: 40 (7%)\n",
      "Unique values in 2: 27 (5%)\n",
      "Unique values in 3: 59 (11%)\n",
      "Unique values in 4: 127 (23%)\n",
      "Unique values in 5: 34 (6%)\n",
      "Unique values in 6: 19 (3%)\n",
      "Unique values in 7: 114 (21%)\n",
      "Unique values in 8: 34 (6%)\n",
      "Unique values in 9: 12 (2%)\n",
      "Unique values in 10: 65 (12%)\n",
      "Unique values in 11: 114 (21%)\n",
      "Unique values in 12: 320 (58%)\n",
      "Unique values in 13: 134 (24%)\n",
      "Unique values in 14: 34 (6%)\n",
      "Unique values in 15: 23 (4%)\n",
      "Unique values in 16: 39 (7%)\n",
      "Unique values in 17: 29 (5%)\n",
      "Unique values in 18: 31 (6%)\n"
     ]
    }
   ],
   "source": [
    "for column in X_train_pd.columns:\n",
    "    unique_count = len(X_train_pd[column].unique())\n",
    "    print('Unique values in {}: {} ({:0.0f}%)'.format(column, unique_count, unique_count / X_train_pd.shape[0] * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Т.к. о природе данных ничего не известно сложно делать однозначный вывод. Учитывая, что все данные - целые числа, можно было бы предположить, что фичи с наименьшим количеством уникальных значений являются категориальными с большим числом различных классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lable Encoding\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(target)\n",
    "y_train = label_encoder.transform(y_train)\n",
    "y_test = label_encoder.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Далее, стандартное шкалирование, т.к. на данный момент все величины имеют разный разброс и среднее. Это может плохо сказаться на качестве линейной модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "standart_scaler = StandardScaler()\n",
    "X_train_standart_scaled = standart_scaler.fit_transform(X_train)\n",
    "X_test_standart_scaled = standart_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minmax scaled train shape: (549, 19)\n",
      "Minmax scaled test shape: (297, 19)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "minmax_scaler = MinMaxScaler()\n",
    "minmax_scaler.fit(data)\n",
    "X_train_minmax_scaled = minmax_scaler.transform(X_train)\n",
    "X_test_minmax_scaled = minmax_scaler.transform(X_test)\n",
    "print(f'Minmax scaled train shape: {X_train_minmax_scaled.shape}')\n",
    "print(f'Minmax scaled test shape: {X_test_minmax_scaled.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of my code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1. Basic logistic regression\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the `train` data (small grid/random search is enough, no need to find the *best* parameters).\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` `tol=1e-3` and ` max_iter=500`.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "My code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-1dd5ad5d0845cbbb",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "def one_vs_all_score(y_pred: list, y_true: list) -> tuple:\n",
    "    '''Mean f1 score for multiclass classification based on one-vs-all strategy'''\n",
    "    labels = list(range(len(label_encoder.classes_)))\n",
    "    f1_scores = [ f1_score(y_pred == i, y_true == i) for i in labels ]\n",
    "    accuracy_scores = [ accuracy_score(y_pred == i, y_true == i) for i in labels ]\n",
    "    current_f1_score = np.mean(f1_scores)\n",
    "    current_accuracy_score = np.mean(accuracy_scores)\n",
    "    return current_f1_score, current_accuracy_score\n",
    "\n",
    "def cross_val_param_tune(model_class: BaseEstimator, grid_params: list, model_params: dict, grid_param_name: str,  \\\n",
    "                         X_train: np.array, X_test: np.array, verbose: bool = True) -> float:\n",
    "    '''Crossval best model param search'''\n",
    "    model_f1_scores = []\n",
    "    model_accuracy_scores = []\n",
    "    skf = StratifiedKFold(\n",
    "        n_splits=5,\n",
    "        shuffle=True,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    for l1_ratio in grid_params:\n",
    "        model_params[grid_param_name] = l1_ratio\n",
    "        model = model_class(**model_params)\n",
    "        k_fold_f1_scores = []\n",
    "        k_fold_accuracy_scores = []\n",
    "\n",
    "        for train_ids, test_ids in skf.split(X_train, y_train):\n",
    "            k_X_train, k_X_test = X_train[train_ids], X_train[test_ids]\n",
    "            k_y_train, k_y_test = y_train[train_ids], y_train[test_ids]\n",
    "            model.fit(k_X_train, k_y_train)\n",
    "            k_y_pred = model.predict(k_X_test)\n",
    "            mean_f1, mean_accuracy = one_vs_all_score(k_y_pred, k_y_test)\n",
    "            k_fold_f1_scores.append(mean_f1)\n",
    "            k_fold_accuracy_scores.append(mean_accuracy)\n",
    "\n",
    "        model_f1_scores.append(np.mean(k_fold_f1_scores))\n",
    "        model_accuracy_scores.append(np.mean(k_fold_accuracy_scores))\n",
    "        \n",
    "    if verbose:\n",
    "        for l1_ratio, mean_f1, mean_accuracy in zip(grid_params, model_f1_scores, model_accuracy_scores):\n",
    "            print(f'{grid_param_name}: {l1_ratio}')\n",
    "            print('\\tF1 score: {:0.5}'.format(mean_f1))\n",
    "            print('\\tAccuracy score: {:0.5}'.format(mean_accuracy))\n",
    "\n",
    "    best_f1_ids = np.argmax(model_f1_scores)\n",
    "    best_accuracy_ids = np.argmax(model_accuracy_scores)\n",
    "    \n",
    "    if verbose:\n",
    "        print(f'Best F1 index: {best_f1_ids}')\n",
    "        print(f'Best Accuracy index: {best_accuracy_ids}')\n",
    "\n",
    "    if best_f1_ids == best_accuracy_ids:\n",
    "        best_l1_ratio = grid_params[best_f1_ids]\n",
    "\n",
    "        if verbose:\n",
    "            print(f'Best {grid_param_name}: {best_l1_ratio}')\n",
    "        \n",
    "        return best_l1_ratio\n",
    "    else:\n",
    "        if verbose:\n",
    "            print(f'Best f1 score id {best_f1_ids}')\n",
    "            print(f'Best accuracy score id {best_accuracy_ids}')\n",
    "            \n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_ratio: 0\n",
      "\tF1 score: 0.74263\n",
      "\tAccuracy score: 0.87522\n",
      "l1_ratio: 0.2\n",
      "\tF1 score: 0.74031\n",
      "\tAccuracy score: 0.87431\n",
      "l1_ratio: 0.5\n",
      "\tF1 score: 0.73942\n",
      "\tAccuracy score: 0.87341\n",
      "l1_ratio: 0.8\n",
      "\tF1 score: 0.74124\n",
      "\tAccuracy score: 0.8734\n",
      "l1_ratio: 1\n",
      "\tF1 score: 0.74213\n",
      "\tAccuracy score: 0.87431\n",
      "Best F1 index: 0\n",
      "Best Accuracy index: 0\n",
      "Best l1_ratio: 0\n"
     ]
    }
   ],
   "source": [
    "grid_params = [0, 0.2, 0.5, 0.8, 1]\n",
    "model_params = {\n",
    "        'multi_class': 'multinomial',\n",
    "        'solver': 'saga',\n",
    "        'tol': 1e-3,\n",
    "        'max_iter': 500,\n",
    "        'penalty': 'elasticnet',\n",
    "        'l1_ratio': None,\n",
    "        'verbose': False,\n",
    "        'random_state': 42,\n",
    "    }\n",
    "best_l1_ratio = cross_val_param_tune(LogisticRegression, grid_params, model_params, 'l1_ratio', X_train_standart_scaled, X_test_standart_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.80555\n",
      "Accuracy score: 0.90067\n"
     ]
    }
   ],
   "source": [
    "best_logreg_params = {\n",
    "    'multi_class': 'multinomial',\n",
    "    'solver': 'saga',\n",
    "    'tol': 1e-3,\n",
    "    'max_iter': 500,\n",
    "    'penalty': 'elasticnet',\n",
    "    'l1_ratio': best_l1_ratio,\n",
    "    'verbose': False,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "logreg_model = LogisticRegression(**best_logreg_params)\n",
    "logreg_model.fit(X_train_standart_scaled, y_train)\n",
    "y_pred = logreg_model.predict(X_test_standart_scaled)\n",
    "mean_f1, mean_accuracy = one_vs_all_score(y_pred, y_test)\n",
    "\n",
    "print('F1 score: {:0.5}'.format(mean_f1))\n",
    "print('Accuracy score: {:0.5}'.format(mean_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You might use this command to install scikit-plot. \n",
    "# Warning, if you a running locally, don't call pip from within jupyter, call it from terminal in the corresponding \n",
    "# virtual environment instead\n",
    "\n",
    "# ! pip install scikit-plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2. PCA: explained variance plot\n",
    "* Apply the PCA to the train part of the data. Build the explaided variance plot. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### My Code\n",
    "\n",
    "Выбирать количество признаков будем так, чтобы они объясняли 99% variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recomended number of components: 11\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "target_variance = 0.99\n",
    "\n",
    "pca_params = {\n",
    "    'random_state': 42,\n",
    "    'n_components': target_variance,\n",
    "}\n",
    "\n",
    "pca = PCA(**pca_params)\n",
    "pca.fit(X_train_minmax_scaled)\n",
    "print(f'Recomended number of components: {pca.n_components_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как видно из вывода выше, для этого необходимо сократить размерность до 11.\n",
    "Попробуем также сделать аналогичные выводы из графика, построенного на основе pca.explained_variance_ratio_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-c6c614740bce090e",
     "locked": false,
     "points": 10,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "target_variance = 0.99\n",
    "\n",
    "pca_params = {\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "pca = PCA(**pca_params)\n",
    "pca.fit(X_train_minmax_scaled)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "xi = np.arange(1, X_train.shape[1] + 1, step=1)\n",
    "y = np.cumsum(pca.explained_variance_ratio_)\n",
    "\n",
    "plt.ylim(0.0,1.1)\n",
    "plt.plot(xi, y, marker='o', linestyle='--', color='b')\n",
    "\n",
    "plt.xlabel('Number of Components')\n",
    "plt.xticks(np.arange(0, X_train.shape[1] + 1, step=1))\n",
    "plt.ylabel('Cumulative variance (%)')\n",
    "plt.title('The number of components needed to explain variance')\n",
    "\n",
    "plt.axhline(y=target_variance, color='r', linestyle='-')\n",
    "plt.text(14, 0.85, f'{int(target_variance * 100)}% cut-off threshold', color = 'red', fontsize=16)\n",
    "\n",
    "ax.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Код для построения графика выше взят из статьи https://www.mikulskibartosz.name/pca-how-to-choose-the-number-of-components/\n",
    "\n",
    "Как видно из графика выше, для объяснения 99% variance, достаточно сократить размерность данных до 11.\n",
    "Этот результат следует как из графика выше, так и из результата автоматического подбора необходимого количества параметров в PCA (см.выше).\n",
    "\n",
    "Итак, оптимальное количество элементов: 11\n",
    "\n",
    "#### End of my code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0c1fe666f52fe53c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.3. PCA trasformation\n",
    "* Select the appropriate number of components. Briefly explain your choice. Should you normalize the data?\n",
    "\n",
    "*Use `fit` and `transform` methods to transform the `train` and `test` parts.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### My code\n",
    "\n",
    "Для обучения PCA будем использовать данные преобразованные так, что диапазон всех признаков равен [0;1], т.к. именно такие данные необходимо передавать в PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-96ab18d96473ef71",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA train shape: (549, 11)\n",
      "PCA test shape: (297, 11)\n"
     ]
    }
   ],
   "source": [
    "pca_params = {\n",
    "    'random_state': 42,\n",
    "    'n_components': 11,\n",
    "}\n",
    "\n",
    "pca = PCA(**pca_params)\n",
    "pca_X_train = pca.fit_transform(X_train_minmax_scaled)\n",
    "pca_X_test = pca.transform(X_test_minmax_scaled)\n",
    "print(f'PCA train shape: {pca_X_train.shape}')\n",
    "print(f'PCA test shape: {pca_X_test.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note: From this point `sklearn` [Pipeline](https://scikit-learn.org/stable/modules/compose.html) might be useful to perform transformations on the data. Refer to the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) for more information.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d28b58a35c94e988",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.4. Logistic regression on PCA-preprocessed data.\n",
    "* Find optimal hyperparameters for logistic regression with cross-validation on the transformed by PCA `train` data.\n",
    "\n",
    "* Estimate the model quality with `f1` and `accuracy` scores.\n",
    "* Plot a ROC-curve for the trained model. For the multiclass case you might use `scikitplot` library (e.g. `scikitplot.metrics.plot_roc(test_labels, predicted_proba)`).\n",
    "\n",
    "*Note: please, use the following hyperparameters for logistic regression: `multi_class='multinomial'`, `solver='saga'` and `tol=1e-3`*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-12d53ea45258fa82",
     "locked": false,
     "points": 5,
     "schema_version": 2,
     "solution": true
    }
   },
   "source": [
    "---\n",
    "\n",
    "#### My code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "\n",
    "def create_pipe_with_logreg_param(**params: float) -> Pipeline:\n",
    "    '''Create Pipeline with LogReg for given l1_ratio'''\n",
    "    logreg_pipe = Pipeline(\n",
    "        steps=[\n",
    "            (\n",
    "                'Minmax_scaler',\n",
    "                MinMaxScaler(),\n",
    "            ),\n",
    "            (\n",
    "                'PCA',\n",
    "                PCA(\n",
    "                    n_components=11,\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),\n",
    "            (\n",
    "                'StandartScaler',\n",
    "                StandardScaler(),\n",
    "            ),\n",
    "            (\n",
    "                'LogReg',\n",
    "                LogisticRegression(**params),\n",
    "            ),\n",
    "        ],\n",
    "        verbose=False,\n",
    "    )\n",
    "    \n",
    "    return logreg_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "l1_ratio: 0\n",
      "\tF1 score: 0.70516\n",
      "\tAccuracy score: 0.85702\n",
      "l1_ratio: 0.2\n",
      "\tF1 score: 0.70275\n",
      "\tAccuracy score: 0.85612\n",
      "l1_ratio: 0.5\n",
      "\tF1 score: 0.70103\n",
      "\tAccuracy score: 0.8552\n",
      "l1_ratio: 0.8\n",
      "\tF1 score: 0.70454\n",
      "\tAccuracy score: 0.85702\n",
      "l1_ratio: 1\n",
      "\tF1 score: 0.70624\n",
      "\tAccuracy score: 0.85793\n",
      "Best F1 index: 4\n",
      "Best Accuracy index: 4\n",
      "Best l1_ratio: 1\n"
     ]
    }
   ],
   "source": [
    "grid_params = [0, 0.2, 0.5, 0.8, 1]\n",
    "model_params = {\n",
    "    'multi_class': 'multinomial',\n",
    "    'solver': 'saga',\n",
    "    'tol': 1e-3,\n",
    "    'max_iter': 500,\n",
    "    'penalty': 'elasticnet',\n",
    "    'l1_ratio': None,\n",
    "    'verbose': False,\n",
    "    'random_state': 42,\n",
    "}\n",
    "best_l1_ratio = cross_val_param_tune(create_pipe_with_logreg_param, grid_params, model_params, 'l1_ratio', X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.80555\n",
      "Accuracy score: 0.85185\n"
     ]
    }
   ],
   "source": [
    "best_logreg_params = {\n",
    "    'multi_class': 'multinomial',\n",
    "    'solver': 'saga',\n",
    "    'tol': 1e-3,\n",
    "    'max_iter': 500,\n",
    "    'penalty': 'elasticnet',\n",
    "    'l1_ratio': best_l1_ratio,\n",
    "    'verbose': False,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "logreg_pipe = create_pipe_with_logreg_param(**best_logreg_params)\n",
    "logreg_pipe.fit(X_train, y_train)\n",
    "y_pred = logreg_pipe.predict(X_test)\n",
    "y_pred_proba = logreg_pipe.predict_proba(X_test)\n",
    "mena_f1, mean_accuracy = one_vs_all_score(y_pred, y_test)\n",
    "\n",
    "print('F1 score: {:0.5}'.format(mean_f1))\n",
    "print('Accuracy score: {:0.5}'.format(mean_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scikitplot.metrics import plot_roc\n",
    "\n",
    "plot_roc(y_test, y_pred_proba)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of my code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-4fbf16c64076e139",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.5. Decision tree\n",
    "* Now train a desicion tree on the same data. Find optimal tree depth (`max_depth`) using cross-validation.\n",
    "\n",
    "* Measure the model quality using the same metrics you used above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### My code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-748ed20b51c67fab",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_depth: 1\n",
      "\tF1 score: 0.26507\n",
      "\tAccuracy score: 0.70041\n",
      "max_depth: 5\n",
      "\tF1 score: 0.62978\n",
      "\tAccuracy score: 0.82691\n",
      "max_depth: 10\n",
      "\tF1 score: 0.6604\n",
      "\tAccuracy score: 0.8333\n",
      "max_depth: 15\n",
      "\tF1 score: 0.67143\n",
      "\tAccuracy score: 0.83967\n",
      "max_depth: None\n",
      "\tF1 score: 0.66955\n",
      "\tAccuracy score: 0.83785\n",
      "Best F1 index: 3\n",
      "Best Accuracy index: 3\n",
      "Best max_depth: 15\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree_grid_params = [1, 5, 10, 15, None]\n",
    "tree_params = {\n",
    "    'random_state': 42,\n",
    "}\n",
    "best_depth = cross_val_param_tune(DecisionTreeClassifier, tree_grid_params, tree_params, 'max_depth', X_train, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score on test: 0.65172\n",
      "Accuracy score on test: 0.81987\n"
     ]
    }
   ],
   "source": [
    "tree_params = {\n",
    "    'random_state': 42,\n",
    "    'max_depth': best_depth,\n",
    "}\n",
    "tree_model = DecisionTreeClassifier(**tree_params)\n",
    "tree_model.fit(X_train, y_train)\n",
    "tree_y_pred = tree_model.predict(X_test)\n",
    "mean_f1, mean_accuracy = one_vs_all_score(tree_y_pred, y_test)\n",
    "\n",
    "print('F1 score on test: {:0.5}'.format(mean_f1))\n",
    "print('Accuracy score on test: {:0.5}'.format(mean_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of my code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9eadd4d8a03ae67a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.6. Bagging.\n",
    "Here starts the ensembling part.\n",
    "\n",
    "First we will use the __Bagging__ approach. Build an ensemble of $N$ algorithms varying N from $N_{min}=2$ to $N_{max}=100$ (with step 5).\n",
    "\n",
    "We will build two ensembles: of logistic regressions and of decision trees.\n",
    "\n",
    "*Comment: each ensemble should be constructed from models of the same family, so logistic regressions should not be mixed up with decision trees.*\n",
    "\n",
    "\n",
    "*Hint 1: To build a __Bagging__ ensebmle varying the ensemble size efficiently you might generate $N_{max}$ subsets of `train` data (of the same size as the original dataset) using bootstrap procedure once. Then you train a new instance of logistic regression/decision tree with optimal hyperparameters you estimated before on each subset (so you train it from scratch). Finally, to get an ensemble of $N$ models you average the $N$ out of $N_{max}$ models predictions.*\n",
    "\n",
    "*Hint 2: sklearn might help you with this taks. Some appropriate function/class might be out there.*\n",
    "\n",
    "* Plot `f1` and `accuracy` scores plots w.r.t. the size of the ensemble.\n",
    "\n",
    "* Briefly analyse the plot. What is the optimal number of algorithms? Explain your answer.\n",
    "\n",
    "* How do you think, are the hyperparameters for the decision trees you found in 2.5 optimal for trees used in ensemble? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### My code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-8fc95a2b206bdae1",
     "locked": false,
     "points": 35,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "N_MIN = 2\n",
    "N_MAX = 100\n",
    "STEP = 5\n",
    "\n",
    "n_grid_param = list(range(N_MIN, N_MAX + 1, STEP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BaggingClassifier(base_estimator=DecisionTreeClassifier(max_depth=15,\n",
       "                                                        random_state=42),\n",
       "                  n_estimators=100, random_state=42)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import BaggingClassifier\n",
    "\n",
    "tree_optimal_params = {\n",
    "    'random_state': 42,\n",
    "    'max_depth': best_depth,\n",
    "}\n",
    "logreg_optimal_params = {\n",
    "    'multi_class': 'multinomial',\n",
    "    'solver': 'saga',\n",
    "    'tol': 1e-3,\n",
    "    'max_iter': 500,\n",
    "    'penalty': 'elasticnet',\n",
    "    'l1_ratio': best_l1_ratio,\n",
    "    'verbose': False,\n",
    "    'random_state': 42,\n",
    "}\n",
    "\n",
    "logreg_bagging_model = BaggingClassifier(\n",
    "    base_estimator=create_pipe_with_logreg_param(**logreg_optimal_params),\n",
    "    n_estimators=N_MAX,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    ")\n",
    "tree_bagging_model = BaggingClassifier(\n",
    "    base_estimator=DecisionTreeClassifier(**tree_optimal_params),\n",
    "    n_estimators=N_MAX,\n",
    "    bootstrap=True,\n",
    "    random_state=42,\n",
    ")\n",
    "logreg_bagging_model.fit(X_train, y_train)\n",
    "tree_bagging_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(42)\n",
    "\n",
    "def bagging_models_proba_predictions(bagging_model: BaggingClassifier, test_data: np.array) -> list:\n",
    "    '''Get all estimators predictions'''\n",
    "    model_pred_probas = []\n",
    "    \n",
    "    for model in bagging_model.estimators_:\n",
    "        model_pred_probas.append(model.predict_proba(test_data))\n",
    "        \n",
    "    return model_pred_probas\n",
    "\n",
    "def get_random_k_predictions(all_proba_predictions: np.array, k: int) -> np.array:\n",
    "    '''Get k random selected models predictions'''\n",
    "    model_ids = random.sample(list(range(N_MAX)), k=k)\n",
    "    k_proba_predictions = np.stack([all_proba_predictions[i] for i in model_ids], axis=2)\n",
    "    return k_proba_predictions.mean(axis=2).argmax(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg_f1_scores = []\n",
    "logreg_accuracy_scores = []\n",
    "all_logreg_proba_preds = bagging_models_proba_predictions(logreg_bagging_model, X_test)\n",
    "\n",
    "for n in n_grid_param:\n",
    "    n_logreg_preds = get_random_k_predictions(all_logreg_proba_preds, n)\n",
    "    mean_f1, mean_accuracy = one_vs_all_score(n_logreg_preds, y_test)\n",
    "    logreg_f1_scores.append(mean_f1)\n",
    "    logreg_accuracy_scores.append(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_grid_param, logreg_f1_scores)\n",
    "plt.show()\n",
    "plt.plot(n_grid_param, logreg_accuracy_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logreg n: 47\n",
      "Best F1 score: 0.72248\n",
      "Best accuracy score: 0.86027\n"
     ]
    }
   ],
   "source": [
    "best_logreg_f1_id = np.argmax(logreg_f1_scores)\n",
    "best_logreg_accuracy_id = np.argmax(logreg_accuracy_scores)\n",
    "best_logreg_n = None\n",
    "\n",
    "if best_logreg_f1_id == best_logreg_accuracy_id:\n",
    "    best_logreg_n = n_grid_param[best_logreg_f1_id]\n",
    "    print(f'Best logreg n: {best_logreg_n}')\n",
    "    print('Best F1 score: {:0.5}'.format(logreg_f1_scores[best_logreg_f1_id]))\n",
    "    print('Best accuracy score: {:0.5}'.format(logreg_accuracy_scores[best_logreg_accuracy_id]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_f1_scores = []\n",
    "tree_accuracy_scores = []\n",
    "all_tree_proba_preds = bagging_models_proba_predictions(tree_bagging_model, X_test)\n",
    "\n",
    "for n in n_grid_param:\n",
    "    n_tree_preds = get_random_k_predictions(all_tree_proba_preds, n)\n",
    "    mean_f1, mean_accuracy = one_vs_all_score(n_tree_preds, y_test)\n",
    "    tree_f1_scores.append(mean_f1)\n",
    "    tree_accuracy_scores.append(mean_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(n_grid_param, tree_f1_scores)\n",
    "plt.show()\n",
    "plt.plot(n_grid_param, tree_accuracy_scores)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best logreg n: 22\n",
      "Best F1 score: 0.75357\n",
      "Best accuracy score: 0.87374\n"
     ]
    }
   ],
   "source": [
    "best_tree_f1_id = np.argmax(tree_f1_scores)\n",
    "best_tree_accuracy_id = np.argmax(tree_accuracy_scores)\n",
    "best_tree_n = None\n",
    "\n",
    "if best_tree_f1_id == best_tree_accuracy_id:\n",
    "    best_tree_n = n_grid_param[best_tree_f1_id]\n",
    "    print(f'Best logreg n: {best_tree_n}')\n",
    "    print('Best F1 score: {:0.5}'.format(tree_f1_scores[best_tree_f1_id]))\n",
    "    print('Best accuracy score: {:0.5}'.format(tree_accuracy_scores[best_tree_accuracy_id]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### End of my code\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-241b7691ab44cbfb",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.7. Random Forest\n",
    "Now we will work with the Random Forest (its `sklearn` implementation).\n",
    "\n",
    "* * Plot `f1` and `accuracy` scores plots w.r.t. the number of trees in Random Forest.\n",
    "\n",
    "* What is the optimal number of trees you've got? Is it different from the optimal number of logistic regressions/decision trees in 2.6? Explain the results briefly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-888755d0f3d91620",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 100/100 [00:13<00:00,  7.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forest best n: 14\n",
      "Forest best F1 score: 0.76915\n",
      "Forest best accuracy score: 0.88215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tqdm import tqdm\n",
    "\n",
    "N_MAX = 100\n",
    "\n",
    "forest_params = {\n",
    "    'random_state': 42,\n",
    "    'n_estimators': None,\n",
    "    'max_depth': None,\n",
    "}\n",
    "forest_f1_scores = []\n",
    "forest_accuracy_scores = []\n",
    "\n",
    "for i in tqdm(range(1, N_MAX + 1)):\n",
    "    forest_params['n_estimators'] = i\n",
    "    forest_model = RandomForestClassifier(**forest_params)\n",
    "    forest_model.fit(X_train, y_train)\n",
    "    forest_pred = forest_model.predict(X_test)\n",
    "    mean_f1, mean_accuracy = one_vs_all_score(forest_pred, y_test)\n",
    "    forest_f1_scores.append(mean_f1)\n",
    "    forest_accuracy_scores.append(mean_accuracy)\n",
    "    \n",
    "forest_best_f1_id = np.argmax(forest_f1_scores)\n",
    "forest_best_accuracy_id = np.argmax(forest_accuracy_scores)\n",
    "forest_best_f1_score = forest_f1_scores[forest_best_f1_id]\n",
    "forest_best_accuracy_score = forest_accuracy_scores[forest_best_accuracy_id]\n",
    "forest_best_n = None\n",
    "\n",
    "if forest_best_f1_id == forest_best_accuracy_id:\n",
    "    forest_best_n = forest_best_f1_id + 1\n",
    "    \n",
    "    print(f'Forest best n: {forest_best_n}')\n",
    "    print('Forest best F1 score: {:0.5}'.format(forest_best_f1_score))\n",
    "    print('Forest best accuracy score: {:0.5}'.format(forest_best_accuracy_score))\n",
    "else:\n",
    "    print('best F1 n != best accuracy n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, N_MAX + 1), forest_f1_scores)\n",
    "plt.title('Forest F1 scores')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1, N_MAX + 1), forest_accuracy_scores)\n",
    "plt.title('Forest accuracy scores')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-99191c0852538d4d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "#### 2.8. Learning curve\n",
    "Your goal is to estimate, how does the model behaviour change with the increase of the `train` dataset size.\n",
    "\n",
    "* Split the training data into 10 equal (almost) parts. Then train the models from above (Logistic regression, Desicion Tree, Random Forest) with optimal hyperparameters you have selected on 1 part, 2 parts (combined, so the train size in increased by 2 times), 3 parts and so on.\n",
    "\n",
    "* Build a plot of `accuracy` and `f1` scores on `test` part, varying the `train` dataset size (so the axes will be score - dataset size.\n",
    "\n",
    "* Analyse the final plot. Can you make any conlusions using it? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "nbgrader": {
     "grade": true,
     "grade_id": "cell-e39bc7e7dff61ff9",
     "locked": false,
     "points": 15,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Create Assignment",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
